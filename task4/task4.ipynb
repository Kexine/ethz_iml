{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler    \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.utils import resample\n",
    "\n",
    "from skimage.io import imread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "seed = 69\n",
    "\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Load raw training and testing data\n",
    "train_raw_triplets = np.loadtxt('drive/My Drive/Colab Notebooks/train_triplets.txt', dtype='str')\n",
    "testing_raw_triplets = np.loadtxt('drive/My Drive/Colab Notebooks/test_triplets.txt', dtype='str')\n",
    "\n",
    "# Split training IDs into training and validation\n",
    "valid_size = 0.1 # percentage of training set to use as validation\n",
    "ID_train, ID_val = train_test_split(train_raw_triplets, test_size = valid_size) # train/val split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new tripelts for validation: ACB\n",
    "ID_val_new = ID_val.copy()\n",
    "ID_val_new[:, 1] = ID_val[:, 2]\n",
    "ID_val_new[:, 2] = ID_val[:, 1]\n",
    "\n",
    "# Add labels to validation data: ABC = 1 and ACB = 0\n",
    "len_data = ID_val.shape[0]\n",
    "labels_1 = np.ones(len_data)\n",
    "labels_0 = 0*np.ones(len_data)\n",
    "# split data into 2 : half is ABC and other half is ACB\n",
    "labels = np.append(labels_1[0:math.floor(len_data/2)], labels_0[math.floor(len_data/2):], axis = 0)\n",
    "\n",
    "# all validation data file names\n",
    "ID_val_all = np.append(ID_val[0:math.floor(len_data/2)], ID_val_new[math.floor(len_data/2):], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data and network parameters\n",
    "num_workers = 5\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "epochs = 10\n",
    "\n",
    "# Classes to turn data into pytorch dataset for dataloaders\n",
    "## train data class\n",
    "class trainID(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, X_ID):\n",
    "        self.X_ID = X_ID\n",
    "        self.X_ID_A = X_ID[:, 0]\n",
    "        self.X_ID_B = X_ID[:, 1]\n",
    "        self.X_ID_C = X_ID[:, 2]\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X_ID_A[index], self.X_ID_B[index], self.X_ID_C[index]\n",
    "        \n",
    "    def __len__ (self):\n",
    "        return self.X_ID.shape[0]\n",
    "\n",
    "## val data class\n",
    "class valID(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, X_ID, y_data):\n",
    "        self.X_ID = X_ID\n",
    "        self.X_ID_A = X_ID[:, 0]\n",
    "        self.X_ID_B = X_ID[:, 1]\n",
    "        self.X_ID_C = X_ID[:, 2]\n",
    "        self.y_data = y_data\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X_ID_A[index], self.X_ID_B[index], self.X_ID_C[index], self.y_data[index]\n",
    "        \n",
    "    def __len__ (self):\n",
    "        return self.X_ID.shape[0]\n",
    "    \n",
    "\n",
    "## test data class\n",
    "class testID(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, X_ID):\n",
    "        self.X_ID = X_ID\n",
    "        self.X_ID_A = X_ID[:, 0]\n",
    "        self.X_ID_B = X_ID[:, 1]\n",
    "        self.X_ID_C = X_ID[:, 2]\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X_ID_A[index], self.X_ID_B[index], self.X_ID_C[index]\n",
    "        \n",
    "    def __len__ (self):\n",
    "        return self.X_ID.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise dataloaders for training and validation sets\n",
    "train_index = trainID(ID_train)\n",
    "train_loader = DataLoader(dataset=train_index, batch_size=batch_size, shuffle=True, num_workers = num_workers, pin_memory=True)\n",
    "\n",
    "val_index = valID(ID_val_all, labels)\n",
    "val_loader = DataLoader(dataset=val_index, batch_size=128, shuffle=True, num_workers = num_workers)\n",
    "\n",
    "# Initialise dataloaders for testing set\n",
    "test_index = testID(testing_raw_triplets)\n",
    "test_loader = DataLoader(dataset=test_index, batch_size=128, shuffle=False, num_workers = num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Creation\n",
    "\n",
    "model = torchvision.models.resnet50(pretrained=True)\n",
    "print(model)\n",
    "\n",
    "def resnet18_new(model):\n",
    "    \"\"\"\n",
    "    Construct a ResNet-18 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    modified_pretrained = nn.Sequential(*list(model.children())[:-1])\n",
    "    for param in modified_pretrained.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    modified_pretrained.add_module(\"embedding\", EmbeddingNet(model))\n",
    "    return modified_pretrained\n",
    "    \n",
    "class TripletNet(nn.Module):\n",
    "    \"\"\"Triplet Network.\"\"\"\n",
    "\n",
    "    def __init__(self, embeddingnet):\n",
    "        \"\"\"Triplet Network Builder.\"\"\"\n",
    "        super(TripletNet, self).__init__()\n",
    "        self.embeddingnet = embeddingnet\n",
    "        #self.classifier = nn.Linear(2000, 1)\n",
    "\n",
    "    def forward(self, a, p, n):\n",
    "        \"\"\"Forward pass.\"\"\"\n",
    "        # anchor\n",
    "        embedded_A = self.embeddingnet(a)\n",
    "\n",
    "        # positive examples\n",
    "        embedded_B = self.embeddingnet(p)\n",
    "\n",
    "        # negative examples\n",
    "        embedded_C = self.embeddingnet(n)\n",
    "        \n",
    "        # classifier\n",
    "        #classi = embedded_A + embedded_B + embedded_C\n",
    "        #classi = self.classifier(classi)\n",
    "        \n",
    "        return embedded_A, embedded_B, embedded_C#, classi\n",
    "\n",
    "\n",
    "class EmbeddingNet(nn.Module):\n",
    "    \"\"\"EmbeddingNet using ResNet-18.\"\"\"\n",
    "\n",
    "    def __init__(self, resnet):\n",
    "        \"\"\"Initialize EmbeddingNet model.\"\"\"\n",
    "        super(EmbeddingNet, self).__init__()\n",
    "\n",
    "        # Everything except the last linear layer\n",
    "        #self.features = nn.Sequential(*list(resnet.children())[:-1])\n",
    "        num_ftrs = resnet.fc.in_features\n",
    "        self.fc1 = nn.Linear(num_ftrs, 1000)\n",
    "        self.fc2 = nn.Linear(1000, 512)\n",
    "        \n",
    "        # Batch Norm\n",
    "        self.bn1 = nn.BatchNorm1d(1000)\n",
    "        \n",
    "        # Drop-out\n",
    "        self.drop1 = nn.Dropout(p=0)\n",
    "\n",
    "    def forward(self, out):\n",
    "        \"\"\"Forward pass of EmbeddingNet.\"\"\"\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.drop1(F.relu(self.bn1(self.fc1(out))))\n",
    "        out = self.fc2(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "net = TripletNet(resnet18_new(model))\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load batch images from image IDs\n",
    "def batch_images(image_ID, train):\n",
    "    if train==True:\n",
    "        transforms = [torchvision.transforms.ToPILImage(),torchvision.transforms.Resize((256,256)),\n",
    "                    torchvision.transforms.RandomCrop(224),\n",
    "                  torchvision.transforms.RandomHorizontalFlip(p=0.5),\n",
    "                  torchvision.transforms.RandomVerticalFlip(p=0.5), \n",
    "                  torchvision.transforms.RandomRotation(degrees=90), \n",
    "                  torchvision.transforms.ToTensor(),\n",
    "                  torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])] # transforms to apply \n",
    "    else:\n",
    "        transforms = [torchvision.transforms.ToPILImage(),\n",
    "                  torchvision.transforms.Resize((224,224)), \n",
    "                  torchvision.transforms.CenterCrop(224),\n",
    "                  torchvision.transforms.ToTensor(),\n",
    "                  torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])] # transforms to apply\n",
    "                  \n",
    "    comp_transforms = torchvision.transforms.Compose(transforms)\n",
    "    batch_img = []\n",
    "    batch_img = torch.tensor(batch_img)\n",
    "    for i in range(len(image_ID)):\n",
    "        img_path = 'drive/My Drive/Colab Notebooks/food/' + image_ID[i] +'.jpg'\n",
    "        image = imread(img_path)\n",
    "        image = comp_transforms(image)\n",
    "        #plt.figure()\n",
    "        #plt.imshow(image.permute(1, 2, 0))\n",
    "        batch_img = torch.cat((batch_img, image), dim = 0)\n",
    "    batch_img = batch_img.reshape(-1, 3, 224, 224)\n",
    "    return batch_img\n",
    "\n",
    "def binary_acc(A, B, C, y_true):\n",
    "    d_ab = torch.norm(A-B, p=2, dim=1)\n",
    "    d_ac = torch.norm(A-C, p=2, dim=1)\n",
    "    diff = (d_ac-d_ab).detach().cpu().numpy()\n",
    "    y_pred = np.ceil(diff.clip(0, 1))\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = net\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "    print('cuda true')\n",
    "\n",
    "# Specify loss function and optimiser\n",
    "criterion = nn.TripletMarginLoss(margin=1.0, p=2.0, reduce=None, reduction='mean')\n",
    "optimizer = optim.Adam((model.parameters()) , lr=learning_rate, weight_decay=0.001)\n",
    "\n",
    "# quick sanity check\n",
    "for A, B, C in train_loader:\n",
    "    print(A)\n",
    "    break\n",
    "    \n",
    "plt.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "valid_loss_min = np.Inf # track change in validation loss\n",
    "valid_accuracy_max = 0.0 # track change in validation accuracy\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    \n",
    "    # keep track of training and validation loss\n",
    "    train_loss = 0.0\n",
    "    train_accuracy = 0.0\n",
    "    valid_loss = 0.0\n",
    "    valid_accuracy = 0.0\n",
    "    \n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    print('TRAINING')\n",
    "    print('wait...')\n",
    "    model.train()\n",
    "    train = True\n",
    "    for A, B, C in train_loader:\n",
    "        # retrieve batch of training images: tensor 25x3x150x224\n",
    "        X_train_A = batch_images(A, train)\n",
    "        X_train_B = batch_images(B, train)\n",
    "        X_train_C = batch_images(C, train)\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            X_train_A=X_train_A.cuda()\n",
    "            X_train_B=X_train_B.cuda()\n",
    "            X_train_C=X_train_C.cuda()\n",
    "              \n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        embedded_A, embedded_B, embedded_C = model(X_train_A, X_train_B, X_train_C)\n",
    "        \n",
    "        # calculate the batch loss\n",
    "        loss = criterion(embedded_A, embedded_B, embedded_C)\n",
    "        \n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # update training loss\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "    # calculate average loss\n",
    "    train_loss = train_loss/len(train_loader)\n",
    "    print('final loss')\n",
    "    print(train_loss)\n",
    "    \n",
    "        \n",
    "    ######################    \n",
    "    # validate the model #\n",
    "    ######################\n",
    "    print('VALIDATION')\n",
    "    print('wait...')\n",
    "    model.eval()\n",
    "    train = False\n",
    "    with torch.no_grad():\n",
    "        for A, B, C, y_val in val_loader:\n",
    "            # retrieve batch of training images: tensor 10x3x150x224\n",
    "            X_val_A = batch_images(A, train)\n",
    "            X_val_B = batch_images(B, train)\n",
    "            X_val_C = batch_images(C, train)\n",
    "            y_val = y_val.reshape(-1,1)\n",
    "\n",
    "            if torch.cuda.is_available():\n",
    "                X_val_A=X_val_A.cuda()\n",
    "                X_val_B=X_val_B.cuda()\n",
    "                X_val_C=X_val_C.cuda()\n",
    "\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            embedded_A, embedded_B, embedded_C = model(X_val_A, X_val_B, X_val_C)\n",
    "            # calculate the batch loss\n",
    "            loss = criterion(embedded_A, embedded_B, embedded_C)\n",
    "\n",
    "            # update average validation loss \n",
    "            valid_loss += loss.item()\n",
    "            # update training accuracy\n",
    "            valid_accuracy += binary_acc(embedded_A, embedded_B, embedded_C, y_val)\n",
    "            \n",
    "    \n",
    "    valid_loss = valid_loss/len(val_loader)\n",
    "    valid_accuracy = valid_accuracy/len(val_loader)\n",
    "    \n",
    "    # print training/validation statistics \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "        epoch, train_loss, valid_loss))\n",
    "    print('Epoch: {} \\tValidation Acc: {:.6f}'.format(\n",
    "        epoch, valid_accuracy))\n",
    "    \n",
    "    # save model if validation loss has decreased\n",
    "    if valid_accuracy >= valid_accuracy_max:\n",
    "        print('Validation acc increased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "        valid_accuracy_max,\n",
    "        valid_accuracy))\n",
    "        torch.save(model.state_dict(), 'drive/My Drive/Colab Notebooks/model_cifar_triplet_50.pt')\n",
    "        valid_accuracy_max = valid_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model with highest accuracy\n",
    "if torch.cuda.is_available():\n",
    "    model.load_state_dict(torch.load('drive/My Drive/Colab Notebooks/model_cifar_triplet_50.pt'))\n",
    "else:\n",
    "    model.load_state_dict(torch.load('drive/My Drive/Colab Notebooks/model_cifar_triplet_50.pt', map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model and save labels\n",
    "\n",
    "y_pred_list = []\n",
    "model.eval()\n",
    "train = False\n",
    "# iterate over test data\n",
    "with torch.no_grad():\n",
    "    for A, B, C in test_loader:\n",
    "        # retrieve batch of training images: tensor 20x9x200x200\n",
    "        # retrieve batch of training images: tensor 10x3x150x224\n",
    "        X_test_A = batch_images(A, train)\n",
    "        X_test_B = batch_images(B, train)\n",
    "        X_test_C = batch_images(C, train)\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            X_test_A=X_test_A.cuda()\n",
    "            X_test_B=X_test_B.cuda()\n",
    "            X_test_C=X_test_C.cuda()\n",
    "        \n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        embedded_A, embedded_B, embedded_C = model(X_test_A, X_test_B, X_test_C)\n",
    "      \n",
    "        # convert output probabilities to predicted class\n",
    "        d_ab = torch.norm(embedded_A-embedded_B, p=2, dim=1)\n",
    "        d_ac = torch.norm(embedded_A-embedded_C, p=2, dim=1)\n",
    "        diff = (d_ac-d_ab).detach().cpu().numpy()\n",
    "        y_test_pred = np.ceil(diff.clip(0, 1))\n",
    "        y_pred_list.append(y_test_pred) \n",
    "        \n",
    "print(y_pred_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshaping step\n",
    "print(np.array(y_pred_list[0][:]).shape)\n",
    "\n",
    "y_pred = []\n",
    "y_pred = np.array(y_pred)\n",
    "for i in range(466):\n",
    "    n_batch = np.array(y_pred_list[i][:]).size\n",
    "    y_pred = np.concatenate((y_pred, y_pred_list[i][:].reshape(n_batch,)), axis = 0)\n",
    "\n",
    "output = pd.DataFrame(np.array([0]), columns=['Predicted Label'])\n",
    "y_pred = (np.array(y_pred)).reshape(-1, 1)\n",
    "\n",
    "for i in range(len(y_pred)):\n",
    "    output.loc[i,'Predicted Label']=y_pred[i]\n",
    "\n",
    "# Save the output (predicted labels for the test features)\n",
    "output.to_csv('drive/My Drive/Colab Notebooks/prediction_triplet_50.csv', index=False, header=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
